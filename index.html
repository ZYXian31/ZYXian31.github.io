<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-CS336-Spring2025-Lec2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/05/CS336-Spring2025-Lec2/" class="article-date">
  <time class="dt-published" datetime="2025-07-05T06:54:38.000Z" itemprop="datePublished">2025-07-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/05/CS336-Spring2025-Lec2/">CS336 Language Modeling :Tensors, Operations &amp; Training</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="课程概述"><a href="#课程概述" class="headerlink" title="课程概述"></a>课程概述</h2><p>本讲座深入探讨了语言模型训练的核心组件，从底层的张量操作到完整的训练循环。重点关注<strong>资源利用</strong>（内存和计算）的精确计算。</p>
<h3 id="主要内容结构"><a href="#主要内容结构" class="headerlink" title="主要内容结构"></a>主要内容结构</h3><ul>
<li><strong>张量基础</strong> → <strong>模型构建</strong> → <strong>优化器</strong> → <strong>训练循环</strong></li>
<li><strong>资源核算</strong>：内存（GB）+ 计算（FLOPs）</li>
</ul>
<hr>
<h2 id="💡-动机：快速计算题"><a href="#💡-动机：快速计算题" class="headerlink" title="💡 动机：快速计算题"></a>💡 动机：快速计算题</h2><h3 id="问题1：训练时间估算"><a href="#问题1：训练时间估算" class="headerlink" title="问题1：训练时间估算"></a>问题1：训练时间估算</h3><p><strong>70B参数模型，15T tokens，1024 H100s需要多长时间？</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">total_flops = <span class="number">6</span> * <span class="number">70e9</span> * <span class="number">15e12</span>  <span class="comment"># 6倍规则</span></span><br><span class="line">mfu = <span class="number">0.5</span>  <span class="comment"># 模型FLOPs利用率</span></span><br><span class="line">flops_per_day = h100_flop_per_sec * mfu * <span class="number">1024</span> * <span class="number">60</span> * <span class="number">60</span> * <span class="number">24</span></span><br><span class="line">days = total_flops / flops_per_day</span><br></pre></td></tr></table></figure>

<h3 id="问题2：内存限制"><a href="#问题2：内存限制" class="headerlink" title="问题2：内存限制"></a>问题2：内存限制</h3><p><strong>8个H100上用AdamW能训练多大的模型？</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">h100_bytes = <span class="number">80e9</span>  <span class="comment"># 80GB</span></span><br><span class="line">bytes_per_parameter = <span class="number">4</span> + <span class="number">4</span> + (<span class="number">4</span> + <span class="number">4</span>)  <span class="comment"># 参数+梯度+优化器状态</span></span><br><span class="line">num_parameters = (h100_bytes * <span class="number">8</span>) / bytes_per_parameter</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="📊-张量与内存"><a href="#📊-张量与内存" class="headerlink" title="📊 张量与内存"></a>📊 张量与内存</h2><h3 id="数据类型对比"><a href="#数据类型对比" class="headerlink" title="数据类型对比"></a>数据类型对比</h3><table>
<thead>
<tr>
<th>类型</th>
<th>字节</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody><tr>
<td><strong>float32</strong></td>
<td>4</td>
<td>高精度，稳定</td>
<td>内存占用大</td>
</tr>
<tr>
<td><strong>float16</strong></td>
<td>2</td>
<td>节省内存</td>
<td>动态范围小，易下溢</td>
</tr>
<tr>
<td><strong>bfloat16</strong></td>
<td>2</td>
<td>与fp32相同动态范围</td>
<td>精度略低</td>
</tr>
<tr>
<td><strong>fp8</strong></td>
<td>1</td>
<td>极省内存</td>
<td>精度最低，需特殊硬件</td>
</tr>
</tbody></table>
<hr>
<h2 id="⚡-计算量核算（FLOPs）"><a href="#⚡-计算量核算（FLOPs）" class="headerlink" title="⚡ 计算量核算（FLOPs）"></a>⚡ 计算量核算（FLOPs）</h2><h3 id="核心公式"><a href="#核心公式" class="headerlink" title="核心公式"></a>核心公式</h3><ul>
<li><strong>前向传播</strong>：<code>2 × (数据点数) × (参数数)</code> FLOPs</li>
<li><strong>反向传播</strong>：<code>4 × (数据点数) × (参数数)</code> FLOPs  </li>
<li><strong>总计</strong>：<code>6 × (数据点数) × (参数数)</code> FLOPs</li>
</ul>
<h3 id="矩阵乘法：A-m×n-×-B-n×p"><a href="#矩阵乘法：A-m×n-×-B-n×p" class="headerlink" title="矩阵乘法：A(m×n) × B(n×p)"></a>矩阵乘法：<code>A(m×n) × B(n×p)</code></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FLOPs = 2 × m × n × p</span><br></pre></td></tr></table></figure>

<h3 id="模型FLOPs利用率（MFU）"><a href="#模型FLOPs利用率（MFU）" class="headerlink" title="模型FLOPs利用率（MFU）"></a>模型FLOPs利用率（MFU）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MFU = (实际 FLOP/s) / (理论峰值 FLOP/s)</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优秀标准</strong>：MFU ≥ 0.5</li>
<li><strong>影响因素</strong>：硬件类型、数据类型、算法优化</li>
</ul>
<hr>
<h2 id="🎯-Einops：优雅的张量操作"><a href="#🎯-Einops：优雅的张量操作" class="headerlink" title="🎯 Einops：优雅的张量操作"></a>🎯 Einops：优雅的张量操作</h2><h3 id="为什么使用Einops？"><a href="#为什么使用Einops？" class="headerlink" title="为什么使用Einops？"></a>为什么使用Einops？</h3><ul>
<li><strong>可读性</strong>：维度有明确语义</li>
</ul>
<h3 id="三大核心操作"><a href="#三大核心操作" class="headerlink" title="三大核心操作"></a>三大核心操作</h3><h4 id="1-einsum：广义矩阵乘法"><a href="#1-einsum：广义矩阵乘法" class="headerlink" title="1. einsum：广义矩阵乘法"></a>1. <code>einsum</code>：广义矩阵乘法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传统方式</span></span><br><span class="line">z = x @ y.transpose(-<span class="number">2</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Einops方式</span></span><br><span class="line">z = einsum(x, y, <span class="string">&quot;batch seq1 hidden, batch seq2 hidden -&gt; batch seq1 seq2&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-reduce：维度约简"><a href="#2-reduce：维度约简" class="headerlink" title="2. reduce：维度约简"></a>2. <code>reduce</code>：维度约简</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传统方式</span></span><br><span class="line">y = x.mean(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Einops方式</span></span><br><span class="line">y = reduce(x, <span class="string">&quot;... hidden -&gt; ...&quot;</span>, <span class="string">&quot;mean&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-rearrange：重新排列"><a href="#3-rearrange：重新排列" class="headerlink" title="3. rearrange：重新排列"></a>3. <code>rearrange</code>：重新排列</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分解维度</span></span><br><span class="line">x = rearrange(x, <span class="string">&quot;... (heads hidden) -&gt; ... heads hidden&quot;</span>, heads=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并维度</span></span><br><span class="line">x = rearrange(x, <span class="string">&quot;... heads hidden -&gt; ... (heads hidden)&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="🧠-模型构建"><a href="#🧠-模型构建" class="headerlink" title="🧠 模型构建"></a>🧠 模型构建</h2><h3 id="参数初始化的关键问题"><a href="#参数初始化的关键问题" class="headerlink" title="参数初始化的关键问题"></a>参数初始化的关键问题</h3><p><strong>问题</strong>：<code>torch.randn(input_dim, output_dim)</code> 会导致输出随输入维度缩放</p>
<p><strong>解决方案</strong>：Xavier初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 标准初始化（避免输出随输入维度缩放）</span></span><br><span class="line">w = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 截断正态分布（更安全）</span></span><br><span class="line">w = nn.Parameter(nn.init.trunc_normal_(</span><br><span class="line">    torch.empty(input_dim, output_dim), </span><br><span class="line">    std=<span class="number">1</span>/np.sqrt(input_dim), </span><br><span class="line">    a=-<span class="number">3</span>, b=<span class="number">3</span>  <span class="comment"># 范围约束在 [-3, 3]</span></span><br><span class="line">))</span><br></pre></td></tr></table></figure>

<h3 id="自定义模型示例"><a href="#自定义模型示例" class="headerlink" title="自定义模型示例"></a>自定义模型示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Cruncher</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dim: <span class="built_in">int</span>, num_layers: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layers = nn.ModuleList([</span><br><span class="line">            Linear(dim, dim) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_layers)</span><br><span class="line">        ])</span><br><span class="line">        <span class="variable language_">self</span>.final = Linear(dim, <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.final(x).squeeze(-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="🔧-优化器演进"><a href="#🔧-优化器演进" class="headerlink" title="🔧 优化器演进"></a>🔧 优化器演进</h2><h3 id="优化器家族树"><a href="#优化器家族树" class="headerlink" title="优化器家族树"></a>优化器家族树</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SGD → SGD + momentum → AdaGrad → RMSProp → Adam</span><br></pre></td></tr></table></figure>

<h3 id="各优化器核心思想"><a href="#各优化器核心思想" class="headerlink" title="各优化器核心思想"></a>各优化器核心思想</h3><h4 id="1-SGD-Stochastic-Gradient-Descent"><a href="#1-SGD-Stochastic-Gradient-Descent" class="headerlink" title="1. SGD (Stochastic Gradient Descent)"></a>1. <strong>SGD (Stochastic Gradient Descent)</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心思想：最简单的梯度下降</span></span><br><span class="line"><span class="comment"># 更新公式：θ = θ - lr * grad</span></span><br><span class="line">p.data -= lr * grad</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：简单，内存效率高</li>
<li><strong>缺点</strong>：收敛慢，对学习率敏感</li>
</ul>
<h4 id="2-SGD-Momentum"><a href="#2-SGD-Momentum" class="headerlink" title="2. SGD + Momentum"></a>2. <strong>SGD + Momentum</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心思想：利用历史梯度信息，加速收敛</span></span><br><span class="line"><span class="comment"># 更新公式：v = β * v + grad; θ = θ - lr * v</span></span><br><span class="line">velocity = beta * velocity + grad</span><br><span class="line">p.data -= lr * velocity</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：收敛更快，能跨越小的局部最优</li>
</ul>
<h4 id="3-AdaGrad"><a href="#3-AdaGrad" class="headerlink" title="3. AdaGrad"></a>3. <strong>AdaGrad</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心思想：自适应学习率，频繁更新的参数学习率逐渐减小</span></span><br><span class="line"><span class="comment"># 更新公式：G = G + grad²; θ = θ - lr * grad / √(G + ε)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：自适应学习率，适合稀疏数据</li>
<li><strong>缺点</strong>：学习率单调递减，可能过早停止学习</li>
</ul>
<h4 id="4-RMSProp"><a href="#4-RMSProp" class="headerlink" title="4. RMSProp"></a>4. <strong>RMSProp</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心思想：修复AdaGrad学习率过度衰减问题</span></span><br><span class="line"><span class="comment"># 更新公式：G = β * G + (1-β) * grad²; θ = θ - lr * grad / √(G + ε)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：解决AdaGrad学习率衰减问题</li>
</ul>
<h4 id="5-Adam"><a href="#5-Adam" class="headerlink" title="5. Adam"></a>5. <strong>Adam</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 核心思想：结合momentum和RMSProp的优点</span></span><br><span class="line"><span class="comment"># 更新公式：m = β₁ * m + (1-β₁) * grad; v = β₂ * v + (1-β₂) * grad²</span></span><br><span class="line"><span class="comment">#          θ = θ - lr * m̂ / (√v̂ + ε)</span></span><br></pre></td></tr></table></figure>
<ul>
<li><strong>优点</strong>：结合了momentum和自适应学习率</li>
<li><strong>缺点</strong>：内存占用较大，有时收敛不如SGD</li>
</ul>
<h3 id="AdaGrad详细实现与解释"><a href="#AdaGrad详细实现与解释" class="headerlink" title="AdaGrad详细实现与解释"></a>AdaGrad详细实现与解释</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdaGrad</span>(torch.optim.Optimizer):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, params, lr=<span class="number">0.01</span>, eps=<span class="number">1e-8</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        AdaGrad优化器初始化</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            params: 模型参数（通常是model.parameters()）</span></span><br><span class="line"><span class="string">            lr: 学习率（默认0.01）</span></span><br><span class="line"><span class="string">            eps: 数值稳定性常数（避免除零错误）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(params, <span class="built_in">dict</span>(lr=lr, eps=eps))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        执行一步参数更新</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        AdaGrad的核心思想：</span></span><br><span class="line"><span class="string">        1. 累积每个参数的历史梯度平方和</span></span><br><span class="line"><span class="string">        2. 用累积的梯度平方和对当前梯度进行归一化</span></span><br><span class="line"><span class="string">        3. 这样频繁更新的参数会有更小的有效学习率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> group <span class="keyword">in</span> <span class="variable language_">self</span>.param_groups:  <span class="comment"># 遍历参数组</span></span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> group[<span class="string">&quot;params&quot;</span>]:    <span class="comment"># 遍历每个参数</span></span><br><span class="line">                <span class="keyword">if</span> p.grad <span class="keyword">is</span> <span class="literal">None</span>:       <span class="comment"># 跳过没有梯度的参数</span></span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 获取参数的状态字典（用于存储优化器状态）</span></span><br><span class="line">                state = <span class="variable language_">self</span>.state[p]</span><br><span class="line">                grad = p.grad.data       <span class="comment"># 当前梯度</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 初始化累积梯度平方和 G_t</span></span><br><span class="line">                <span class="comment"># 这个变量存储从训练开始到现在所有梯度的平方和</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(state) == <span class="number">0</span>:</span><br><span class="line">                    state[<span class="string">&quot;sum_of_squares&quot;</span>] = torch.zeros_like(grad)</span><br><span class="line">                </span><br><span class="line">                sum_of_squares = state[<span class="string">&quot;sum_of_squares&quot;</span>]</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 累积当前梯度的平方：G_t = G_&#123;t-1&#125; + grad_t²</span></span><br><span class="line">                <span class="comment"># 这是AdaGrad的核心：记录每个参数的历史梯度信息</span></span><br><span class="line">                sum_of_squares.add_(grad.<span class="built_in">pow</span>(<span class="number">2</span>))</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 计算自适应学习率：lr / √(G_t + ε)</span></span><br><span class="line">                <span class="comment"># eps防止除零错误，通常设为1e-8</span></span><br><span class="line">                adaptive_lr = group[<span class="string">&quot;lr&quot;</span>] / (sum_of_squares.sqrt().add_(group[<span class="string">&quot;eps&quot;</span>]))</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 更新参数：θ_t = θ_&#123;t-1&#125; - (lr / √(G_t + ε)) * grad_t</span></span><br><span class="line">                <span class="comment"># 注意：频繁更新的参数（G_t较大）会有更小的有效学习率</span></span><br><span class="line">                p.data.add_(grad * adaptive_lr, alpha=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="AdaGrad工作原理深度解析"><a href="#AdaGrad工作原理深度解析" class="headerlink" title="AdaGrad工作原理深度解析"></a>AdaGrad工作原理深度解析</h3><h4 id="数学原理"><a href="#数学原理" class="headerlink" title="数学原理"></a>数学原理</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">G_t = G_&#123;t-1&#125; + grad_t²                    # 累积梯度平方和</span><br><span class="line">θ_t = θ_&#123;t-1&#125; - (lr / √(G_t + ε)) * grad_t # 参数更新</span><br></pre></td></tr></table></figure>

<h4 id="关键特性"><a href="#关键特性" class="headerlink" title="关键特性"></a>关键特性</h4><ol>
<li><strong>自适应学习率</strong>：每个参数都有独立的学习率</li>
<li><strong>频率敏感</strong>：更新频繁的参数学习率逐渐减小</li>
</ol>
<h4 id="优缺点分析"><a href="#优缺点分析" class="headerlink" title="优缺点分析"></a>优缺点分析</h4><p><strong>优点</strong>：</p>
<ul>
<li>无需手动调整学习率</li>
<li>对稀疏数据表现优异</li>
<li>理论保证收敛</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>学习率单调递减（G_t只增不减）</li>
<li>可能过早停止学习</li>
</ul>
<hr>
<h2 id="📈-训练循环与最佳实践"><a href="#📈-训练循环与最佳实践" class="headerlink" title="📈 训练循环与最佳实践"></a>📈 训练循环与最佳实践</h2><h3 id="完整训练循环"><a href="#完整训练循环" class="headerlink" title="完整训练循环"></a>完整训练循环</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_step</span>(<span class="params">model, optimizer, get_batch, B</span>):</span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    x, y = get_batch(B)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    pred_y = model(x)</span><br><span class="line">    loss = F.mse_loss(pred_y, y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> loss.item()</span><br></pre></td></tr></table></figure>

<h3 id="关键最佳实践"><a href="#关键最佳实践" class="headerlink" title="关键最佳实践"></a>关键最佳实践</h3><h4 id="1-随机性控制"><a href="#1-随机性控制" class="headerlink" title="1. 随机性控制"></a>1. 随机性控制</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">set_seed</span>(<span class="params">seed=<span class="number">0</span></span>):</span><br><span class="line">    torch.manual_seed(seed)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    random.seed(seed)</span><br></pre></td></tr></table></figure>

<h4 id="2-大数据处理"><a href="#2-大数据处理" class="headerlink" title="2. 大数据处理"></a>2. 大数据处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用memory mapping避免一次性加载</span></span><br><span class="line">data = np.memmap(<span class="string">&quot;data.npy&quot;</span>, dtype=np.int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步数据传输</span></span><br><span class="line">x = x.pin_memory().to(device, non_blocking=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-检查点保存"><a href="#3-检查点保存" class="headerlink" title="3. 检查点保存"></a>3. 检查点保存</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">checkpoint = &#123;</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: model.state_dict(),</span><br><span class="line">    <span class="string">&quot;optimizer&quot;</span>: optimizer.state_dict(),</span><br><span class="line">    <span class="string">&quot;step&quot;</span>: step,</span><br><span class="line">    <span class="string">&quot;loss&quot;</span>: loss</span><br><span class="line">&#125;</span><br><span class="line">torch.save(checkpoint, <span class="string">&quot;checkpoint.pt&quot;</span>)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="🎨-混合精度训练"><a href="#🎨-混合精度训练" class="headerlink" title="🎨 混合精度训练"></a>🎨 混合精度训练</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p><strong>默认高精度，如果降低精度没有影响则降低精度</strong></p>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><ul>
<li><strong>前向传播</strong>：使用 bfloat16&#x2F;fp8（节省内存和计算）</li>
<li><strong>参数存储</strong>：使用 float32（保证精度）</li>
<li><strong>梯度计算</strong>：使用 float32（保证稳定性）</li>
</ul>
<h3 id="PyTorch-AMP-实现"><a href="#PyTorch-AMP-实现" class="headerlink" title="PyTorch AMP 实现"></a>PyTorch AMP 实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> autocast():</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line">scaler.scale(loss).backward()</span><br><span class="line">scaler.step(optimizer)</span><br><span class="line">scaler.update()</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="📊-资源核算总结"><a href="#📊-资源核算总结" class="headerlink" title="📊 资源核算总结"></a>📊 资源核算总结</h2><h3 id="内存组成"><a href="#内存组成" class="headerlink" title="内存组成"></a>内存组成</h3><ol>
<li><strong>模型参数</strong> </li>
<li><strong>优化器状态</strong>（通常与参数同量级）</li>
<li><strong>梯度</strong>（与参数同量级）</li>
<li><strong>激活值</strong>（取决于batch size和序列长度）</li>
</ol>
<h3 id="计算量估算"><a href="#计算量估算" class="headerlink" title="计算量估算"></a>计算量估算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个训练步骤的总FLOPs</span></span><br><span class="line">total_flops = <span class="number">6</span> * batch_size * seq_len * num_parameters</span><br><span class="line"></span><br><span class="line"><span class="comment"># 考虑实际利用率</span></span><br><span class="line">effective_flops = total_flops * mfu</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="🔗-延伸阅读"><a href="#🔗-延伸阅读" class="headerlink" title="🔗 延伸阅读"></a>🔗 延伸阅读</h2><h3 id="核心资源"><a href="#核心资源" class="headerlink" title="核心资源"></a>核心资源</h3><ul>
<li><a target="_blank" rel="noopener" href="https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf">Assignment 1 Handout</a></li>
<li><a target="_blank" rel="noopener" href="https://johnthickstun.com/docs/transformers.pdf">Mathematical Transformers</a></li>
<li><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">Illustrated Transformer</a></li>
</ul>
<h3 id="深入理解"><a href="#深入理解" class="headerlink" title="深入理解"></a>深入理解</h3><ul>
<li><a target="_blank" rel="noopener" href="https://erees.dev/transformer-memory/">Transformer内存使用详解</a></li>
<li><a target="_blank" rel="noopener" href="https://www.adamcasson.com/posts/transformer-flops">Transformer FLOPs计算</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4">语言模型训练的FLOPs计算</a></li>
</ul>
<hr>
<h2 id="🎯-关键要点"><a href="#🎯-关键要点" class="headerlink" title="🎯 关键要点"></a>🎯 关键要点</h2><ol>
<li><strong>资源核算是必备技能</strong>：训练大模型前必须精确计算内存和计算需求</li>
<li><strong>矩阵乘法主导计算</strong>：优化重点应放在线性层</li>
<li><strong>混合精度是趋势</strong>：在保证稳定性的前提下最大化效率</li>
<li><strong>工程实践同样重要</strong>：检查点、数据加载、随机性控制都是成功训练的关键</li>
</ol>
<blockquote>
<p>💡 <strong>记住</strong>：Modern AI训练不仅是算法问题，更是工程优化问题！</p>
</blockquote>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/05/CS336-Spring2025-Lec2/" data-id="cmcq7e21b0000hqlhdfay1ypn" data-title="CS336 Language Modeling :Tensors, Operations &amp; Training" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-CS336-Spring2025-Lec3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/05/CS336-Spring2025-Lec3/" class="article-date">
  <time class="dt-published" datetime="2025-07-05T06:54:38.000Z" itemprop="datePublished">2025-07-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/05/CS336-Spring2025-Lec3/">CS336 Lecture 3:LM架构与训练 - 学习笔记</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <blockquote>
<p><strong>课程主题：</strong> 深入分析大型语言模型的架构设计与训练技术<br><strong>学习方式：</strong> 通过实际部署模型的配置学习最佳实践<br><strong>日期：</strong> 2025-07-05</p>
</blockquote>
<hr>
<h2 id="📋-课程概述"><a href="#📋-课程概述" class="headerlink" title="📋 课程概述"></a>📋 课程概述</h2><p>本节课深入探讨了大型语言模型（LLM）的架构设计和训练过程中的关键技术选择。通过分析多个主流模型的实际配置，总结出架构设计的共性与差异，为实际开发提供指导。</p>
<h3 id="🎯-主要内容结构"><a href="#🎯-主要内容结构" class="headerlink" title="🎯 主要内容结构"></a>🎯 主要内容结构</h3><ol>
<li><strong>Transformer架构回顾与现代变体</strong></li>
<li><strong>架构组件的设计选择</strong></li>
<li><strong>超参数设置的经验法则</strong></li>
<li><strong>训练稳定性技巧</strong></li>
<li><strong>注意力机制的优化变体</strong></li>
</ol>
<hr>
<h2 id="🏗️-内容-1-Transformer架构基础"><a href="#🏗️-内容-1-Transformer架构基础" class="headerlink" title="🏗️ 内容 1: Transformer架构基础"></a>🏗️ 内容 1: Transformer架构基础</h2><h3 id="1-1-标准Transformer-vs-现代变体"><a href="#1-1-标准Transformer-vs-现代变体" class="headerlink" title="1.1 标准Transformer vs 现代变体"></a>1.1 标准Transformer vs 现代变体</h3><p><strong>原始Transformer的特点：</strong></p>
<ul>
<li>位置编码：正弦余弦函数</li>
<li>前馈网络：ReLU激活</li>
<li>归一化：Post-norm LayerNorm（在残差连接后面）</li>
</ul>
<p><strong>现代实现的改进：</strong></p>
<ul>
<li><strong>Pre-norm结构</strong>：在残差连接之前在多头注意力计算之前进行归一化</li>
<li><strong>RoPE位置编码</strong>：旋转位置嵌入</li>
<li><strong>SwiGLU激活</strong>：替代ReLU</li>
<li><strong>无偏置项</strong>：线性层和LayerNorm不使用bias</li>
</ul>
<h3 id="1-2-架构选择的考量原则"><a href="#1-2-架构选择的考量原则" class="headerlink" title="1.2 架构选择的考量原则"></a>1.2 架构选择的考量原则</h3><ul>
<li>从实际部署的大模型中学习经验</li>
<li>重视计算效率与性能的平衡</li>
<li>考虑训练稳定性和可扩展性</li>
</ul>
<hr>
<h2 id="🔧-内容-2-关键架构组件设计"><a href="#🔧-内容-2-关键架构组件设计" class="headerlink" title="🔧 内容 2: 关键架构组件设计"></a>🔧 内容 2: 关键架构组件设计</h2><h3 id="2-1-归一化层设计"><a href="#2-1-归一化层设计" class="headerlink" title="2.1 归一化层设计"></a>2.1 归一化层设计</h3><h4 id="Pre-norm-vs-Post-norm"><a href="#Pre-norm-vs-Post-norm" class="headerlink" title="Pre-norm vs Post-norm"></a>Pre-norm vs Post-norm</h4><p><strong>Pre-norm结构：</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x → LayerNorm → MultiHeadAttention → Add → LayerNorm → FFN → Add</span><br></pre></td></tr></table></figure></p>
<p><strong>Post-norm结构：</strong><br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x → MultiHeadAttention → Add → LayerNorm → FFN → Add → LayerNorm</span><br></pre></td></tr></table></figure></p>
<p><strong>Pre-norm优势：</strong></p>
<ul>
<li>保持残差连接的主要信号路径（原始的 x 一直贯穿整个通路）</li>
<li>梯度传播更稳定，减少梯度爆炸（PostNorm 深层的梯度更大，训练更不稳定）</li>
<li>支持更大的学习率</li>
<li>几乎所有现代LM都采用Pre-norm（除了OPT350M）</li>
</ul>
<h4 id="LayerNorm-vs-RMSNorm"><a href="#LayerNorm-vs-RMSNorm" class="headerlink" title="LayerNorm vs RMSNorm"></a>LayerNorm vs RMSNorm</h4><p><strong>LayerNorm公式：</strong></p>
<script type="math/tex; mode=display">\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \varepsilon}} \cdot \gamma + \beta</script><p>其中：$\mu = \frac{1}{d}\sum<em>{i=1}^{d} x_i$，$\sigma^2 = \frac{1}{d}\sum</em>{i=1}^{d} (x_i - \mu)^2$</p>
<p><strong>RMSNorm公式：</strong></p>
<script type="math/tex; mode=display">\text{RMSNorm}(x) = \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \varepsilon}} \cdot \gamma</script><p><strong>RMSNorm的优势：</strong></p>
<ul>
<li>计算更高效（不需要计算均值和偏置项）</li>
<li>效果与LayerNorm相当</li>
<li>参数更少（无bias项β）</li>
<li>内存访问模式更优</li>
</ul>
<blockquote>
<p><strong>性能对比：</strong> 虽然transformer中norm计算的FLOPS占比很小（0.17%），但runtime占比达25.5%，因为占据了大量的内存访问</p>
</blockquote>
<p><strong>使用情况：</strong></p>
<ul>
<li><strong>LayerNorm：</strong> GPT1/2/3, OPT, GPT-J, BLOOM</li>
<li><strong>RMSNorm：</strong> LLaMA系列, PaLM, Chinchilla, T5</li>
</ul>
<h3 id="2-2-前馈网络-FFN-设计"><a href="#2-2-前馈网络-FFN-设计" class="headerlink" title="2.2 前馈网络(FFN)设计"></a>2.2 前馈网络(FFN)设计</h3><h4 id="标准FFN与无偏置设计"><a href="#标准FFN与无偏置设计" class="headerlink" title="标准FFN与无偏置设计"></a>标准FFN与无偏置设计</h4><p><strong>原始Transformer FFN：</strong></p>
<script type="math/tex; mode=display">\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2</script><p><strong>现代实现（无偏置）：</strong></p>
<script type="math/tex; mode=display">\text{FFN}(x) = \sigma(xW_1)W_2</script><p><strong>优势：</strong></p>
<ul>
<li>内存效率更高</li>
<li>优化稳定性更好</li>
<li>参数量减少</li>
</ul>
<h3 id="2-3-激活函数选择"><a href="#2-3-激活函数选择" class="headerlink" title="2.3 激活函数选择"></a>2.3 激活函数选择</h3><h4 id="主流激活函数对比"><a href="#主流激活函数对比" class="headerlink" title="主流激活函数对比"></a>主流激活函数对比</h4><p><strong>ReLU：</strong></p>
<script type="math/tex; mode=display">\text{FFN}(x) = \max(0, xW_1)W_2</script><p><strong>GeLU：</strong></p>
<script type="math/tex; mode=display">\text{FFN}(x) = \text{GELU}(xW_1)W_2</script><script type="math/tex; mode=display">\text{GELU}(x) = x \cdot \Phi(x) = x \cdot \frac{1}{2}\left[1 + \text{erf}\left(\frac{x}{\sqrt{2}}\right)\right]</script><p><strong>SwiGLU：</strong></p>
<script type="math/tex; mode=display">\text{Swish}(x) = x \cdot \text{sigmoid}(x) = \frac{x}{1 + e^{-x}}</script><h4 id="门控线性单元-GLU"><a href="#门控线性单元-GLU" class="headerlink" title="门控线性单元(GLU)"></a>门控线性单元(GLU)</h4><p><strong>基本GLU思想：</strong></p>
<script type="math/tex; mode=display">\text{GLU}(x) = \sigma(xW_1) \odot (xV)</script><p><strong>ReGLU：</strong></p>
<script type="math/tex; mode=display">\text{FFN}_{\text{ReGLU}}(x) = (\max(0, xW_1) \odot xV)W_2</script><p><strong>SwiGLU：</strong></p>
<script type="math/tex; mode=display">\text{FFN}_{\text{SwiGLU}}(x) = (\text{Swish}(xW_1) \odot xV)W_2</script><p><strong>GeGLU：</strong></p>
<script type="math/tex; mode=display">\text{FFN}_{\text{GeGLU}}(x) = (\text{GELU}(xW_1) \odot xV)W_2</script><p><strong>性能表现：</strong></p>
<ul>
<li>GLU变体通常带来一致的性能提升</li>
<li>门控模型的 $d_{\text{ff}}$ 维度通常缩放为原来的 $\frac{2}{3}$</li>
</ul>
<h3 id="2-4-层结构：串行-vs-并行"><a href="#2-4-层结构：串行-vs-并行" class="headerlink" title="2.4 层结构：串行 vs 并行"></a>2.4 层结构：串行 vs 并行</h3><p><strong>传统串行结构：</strong></p>
<script type="math/tex; mode=display">y = x + \text{MLP}(\text{LayerNorm}(x + \text{Attention}(\text{LayerNorm}(x))))</script><p><strong>并行结构：</strong></p>
<script type="math/tex; mode=display">y = x + \text{MLP}(\text{LayerNorm}(x)) + \text{Attention}(\text{LayerNorm}(x))</script><p><strong>并行结构优势：</strong></p>
<ul>
<li>LayerNorm可以共享</li>
<li>矩阵乘法可以融合</li>
<li>计算效率提升</li>
<li>使用模型：GPT-J, PaLM, GPT-NeoX</li>
</ul>
<blockquote>
<p><strong>注意：</strong> 最近的研究工作多用串行而不是并行</p>
</blockquote>
<hr>
<h2 id="📍-内容-3-位置编码技术"><a href="#📍-内容-3-位置编码技术" class="headerlink" title="📍 内容 3: 位置编码技术"></a>📍 内容 3: 位置编码技术</h2><h3 id="3-1-位置编码类型对比"><a href="#3-1-位置编码类型对比" class="headerlink" title="3.1 位置编码类型对比"></a>3.1 位置编码类型对比</h3><p><strong>正弦编码：</strong></p>
<script type="math/tex; mode=display">\text{PE}(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)</script><script type="math/tex; mode=display">\text{PE}(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)</script><p><strong>绝对位置嵌入：</strong></p>
<script type="math/tex; mode=display">\text{Embed}(x, i) = v_x + u_i</script><p><strong>相对位置嵌入：</strong></p>
<script type="math/tex; mode=display">\text{Embed}(x, i) = v_x + \text{RelativeEmbedding}(i-j)</script><h3 id="3-2-RoPE（旋转位置嵌入）"><a href="#3-2-RoPE（旋转位置嵌入）" class="headerlink" title="3.2 RoPE（旋转位置嵌入）"></a>3.2 RoPE（旋转位置嵌入）</h3><p><strong>设计理念：</strong><br>相对位置嵌入应满足：</p>
<script type="math/tex; mode=display">\langle f(x,i), f(y,j) \rangle = g(x,y,i-j)</script><p><strong>RoPE的数学实现：</strong></p>
<p>对于2D情况，旋转矩阵为：</p>
<script type="math/tex; mode=display">R_{\theta} = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix}</script><p>对于位置 $m$ 的查询向量 $q_m$ 和位置 $n$ 的键向量 $k_n$：</p>
<script type="math/tex; mode=display">q_m = R_{\theta_m} \cdot W_q x_m</script><script type="math/tex; mode=display">k_n = R_{\theta_n} \cdot W_k x_n</script><p>其中 $\theta_i = \frac{i}{10000^{2j/d}}$ 对于维度 $j$</p>
<p><strong>内积计算：</strong></p>
<script type="math/tex; mode=display">\langle q_m, k_n \rangle = \langle R_{\theta_m} W_q x_m, R_{\theta_n} W_k x_n \rangle = \langle W_q x_m, R_{\theta_{n-m}} W_k x_n \rangle</script><p><strong>优势：</strong></p>
<ul>
<li>真正的相对位置感知</li>
<li>避免了正弦编码的交叉项问题</li>
<li>大多数2024+模型的标准选择</li>
</ul>
<hr>
<h2 id="📊-内容-4-超参数设计经验"><a href="#📊-内容-4-超参数设计经验" class="headerlink" title="📊 内容 4: 超参数设计经验"></a>📊 内容 4: 超参数设计经验</h2><h3 id="4-1-前馈维度比例"><a href="#4-1-前馈维度比例" class="headerlink" title="4.1 前馈维度比例"></a>4.1 前馈维度比例</h3><h4 id="标准比例关系"><a href="#标准比例关系" class="headerlink" title="标准比例关系"></a>标准比例关系</h4><p><strong>基本经验法则：</strong></p>
<script type="math/tex; mode=display">d_{\text{ff}} = 4 \times d_{\text{model}}</script><p><strong>GLU变体的调整：</strong></p>
<script type="math/tex; mode=display">d_{\text{ff}} = \frac{8}{3} \times d_{\text{model}} \approx 2.67 \times d_{\text{model}}</script><p><strong>实际模型案例：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>$d<em>{\text{ff}}/d</em>{\text{model}}$</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>PaLM</td>
<td>4.0</td>
<td>标准比例</td>
</tr>
<tr>
<td>Mistral 7B</td>
<td>3.5</td>
<td>GLU变体</td>
</tr>
<tr>
<td>LLaMA-2 70B</td>
<td>3.5</td>
<td>GLU变体</td>
</tr>
<tr>
<td>LLaMA 70B</td>
<td>2.68</td>
<td>GLU变体</td>
</tr>
<tr>
<td>T5 11B</td>
<td>64.0</td>
<td>极端案例</td>
</tr>
<tr>
<td>T5 v1.1</td>
<td>2.5</td>
<td>修正后</td>
</tr>
</tbody>
</table>
</div>
<p><strong>特殊案例分析：</strong></p>
<ul>
<li>T5 11B模型：$d<em>{\text{ff}} = 65536$，$d</em>{\text{model}} = 1024$，比例为64倍</li>
<li>经验表明1-10倍的范围内性能相对稳定</li>
</ul>
<h3 id="4-2-注意力头配置"><a href="#4-2-注意力头配置" class="headerlink" title="4.2 注意力头配置"></a>4.2 注意力头配置</h3><h4 id="头维度设计原则"><a href="#头维度设计原则" class="headerlink" title="头维度设计原则"></a>头维度设计原则</h4><p><strong>标准配置：</strong></p>
<script type="math/tex; mode=display">\text{head\_dim} \times \text{num\_heads} = d_{\text{model}}</script><p><strong>实际案例：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>头数</th>
<th>头维度</th>
<th>模型维度</th>
<th>比例</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3</td>
<td>96</td>
<td>128</td>
<td>12288</td>
<td>1.0</td>
</tr>
<tr>
<td>T5</td>
<td>128</td>
<td>128</td>
<td>1024</td>
<td>16.0</td>
</tr>
<tr>
<td>LLaMA-2</td>
<td>64</td>
<td>128</td>
<td>8192</td>
<td>1.0</td>
</tr>
<tr>
<td>PaLM</td>
<td>48</td>
<td>258</td>
<td>18432</td>
<td>1.48</td>
</tr>
</tbody>
</table>
</div>
<p><strong>设计考虑：</strong></p>
<ul>
<li>大多数模型采用1:1的比例</li>
<li>Google的某些模型采用更高的比例</li>
</ul>
<h3 id="4-3-模型深宽比"><a href="#4-3-模型深宽比" class="headerlink" title="4.3 模型深宽比"></a>4.3 模型深宽比</h3><h4 id="典型深宽比范围"><a href="#典型深宽比范围" class="headerlink" title="典型深宽比范围"></a>典型深宽比范围</h4><p><strong>模型维度/层数比例：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>$d<em>{\text{model}}/n</em>{\text{layer}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td>BLOOM</td>
<td>205</td>
</tr>
<tr>
<td>T5 v1.1</td>
<td>171</td>
</tr>
<tr>
<td>PaLM (540B)</td>
<td>156</td>
</tr>
<tr>
<td>GPT-3/OPT/Mistral/Qwen</td>
<td>128</td>
</tr>
<tr>
<td>LLaMA/LLaMA-2/Chinchilla</td>
<td>102</td>
</tr>
<tr>
<td>T5 (11B)</td>
<td>43</td>
</tr>
<tr>
<td>GPT-2</td>
<td>33</td>
</tr>
</tbody>
</table>
</div>
<p><strong>设计考虑：</strong></p>
<ul>
<li>极深模型并行化困难，延迟较高</li>
<li>100-200的比例范围通常表现良好</li>
</ul>
<h3 id="4-4-词表大小设计"><a href="#4-4-词表大小设计" class="headerlink" title="4.4 词表大小设计"></a>4.4 词表大小设计</h3><p><strong>单语言模型：</strong> 30-50K词汇</p>
<ul>
<li>GPT-2/3: 50,257</li>
<li>LLaMA: 32,000</li>
<li>T5: 32,128</li>
</ul>
<p><strong>多语言模型：</strong> 100-250K词汇</p>
<ul>
<li>mT5: 250,000</li>
<li>PaLM: 256,000</li>
<li>GPT-4: 100,276</li>
</ul>
<h3 id="4-5-正则化策略"><a href="#4-5-正则化策略" class="headerlink" title="4.5 正则化策略"></a>4.5 正则化策略</h3><h4 id="Dropout使用趋势"><a href="#Dropout使用趋势" class="headerlink" title="Dropout使用趋势"></a>Dropout使用趋势</h4><p><strong>历史vs现代：</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>Dropout</th>
<th>Weight Decay</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-2</td>
<td>0.1</td>
<td>0.1</td>
<td>早期模型</td>
</tr>
<tr>
<td>T5</td>
<td>0.1</td>
<td>0</td>
<td>早期模型</td>
</tr>
<tr>
<td>T5 v1.1</td>
<td>0</td>
<td>0</td>
<td>现代趋势</td>
</tr>
<tr>
<td>LLaMA</td>
<td>0</td>
<td>0.1</td>
<td>现代趋势</td>
</tr>
<tr>
<td>PaLM</td>
<td>0</td>
<td>变化</td>
<td>现代趋势</td>
</tr>
</tbody>
</table>
</div>
<h4 id="权重衰减的作用"><a href="#权重衰减的作用" class="headerlink" title="权重衰减的作用"></a>权重衰减的作用</h4><p><strong>数学表示：</strong></p>
<script type="math/tex; mode=display">\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{original}} + \lambda \sum_{i} \|\theta_i\|^2</script><p><strong>作用机制：</strong></p>
<ul>
<li>主要影响优化动态，而非过拟合控制</li>
<li>与学习率调度（如余弦调度）存在交互作用</li>
<li>相同权重衰减设置在不同学习率调度下效果不同</li>
</ul>
<hr>
<h2 id="🛡️-内容-5-训练稳定性技巧"><a href="#🛡️-内容-5-训练稳定性技巧" class="headerlink" title="🛡️ 内容 5: 训练稳定性技巧"></a>🛡️ 内容 5: 训练稳定性技巧</h2><h3 id="5-1-Softmax稳定性问题"><a href="#5-1-Softmax稳定性问题" class="headerlink" title="5.1 Softmax稳定性问题"></a>5.1 Softmax稳定性问题</h3><h4 id="标准Softmax"><a href="#标准Softmax" class="headerlink" title="标准Softmax"></a>标准Softmax</h4><p><strong>计算公式：</strong></p>
<script type="math/tex; mode=display">\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}</script><p><strong>数值稳定版本：</strong></p>
<script type="math/tex; mode=display">\text{softmax}(x_i) = \frac{e^{x_i - \max(x)}}{\sum_{j=1}^{n} e^{x_j - \max(x)}}</script><h4 id="Z-loss技术"><a href="#Z-loss技术" class="headerlink" title="Z-loss技术"></a>Z-loss技术</h4><p><strong>问题：</strong> 输出softmax可能数值不稳定</p>
<p><strong>原始损失：</strong></p>
<script type="math/tex; mode=display">\log P(x) = U(x) - \log Z(x)</script><p><strong>添加Z-loss后：</strong></p>
<script type="math/tex; mode=display">\log P(x) = U(x) - \alpha \log Z(x)</script><p>其中 $Z(x) = \sum_{j} e^{U_j(x)}$，PaLM设置 $\alpha = 10^{-4}$</p>
<p><strong>目标：</strong> 鼓励softmax将 $\log(Z)$ 正则化接近0，提升训练稳定性</p>
<h4 id="QK归一化"><a href="#QK归一化" class="headerlink" title="QK归一化"></a>QK归一化</h4><p><strong>方法：</strong> 在query和key进行矩阵乘法前分别进行LayerNorm</p>
<p><strong>实现：</strong></p>
<script type="math/tex; mode=display">\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{\text{LayerNorm}(Q) \cdot \text{LayerNorm}(K)^T}{\sqrt{d_k}}\right) V</script><p><strong>使用模型：</strong> DCLM, OLMo2, Gemma 2</p>
<h3 id="5-2-Logit软截断"><a href="#5-2-Logit软截断" class="headerlink" title="5.2 Logit软截断"></a>5.2 Logit软截断</h3><p><strong>技术实现：</strong></p>
<script type="math/tex; mode=display">\text{logits}_{\text{capped}} = \text{soft\_cap} \cdot \tanh\left(\frac{\text{logits}}{\text{soft\_cap}}\right)</script><p><strong>目的：</strong> 防止logits过大导致的数值问题<br><strong>权衡：</strong> 可能影响模型性能</p>
<hr>
<h2 id="⚡-内容-6-注意力机制优化"><a href="#⚡-内容-6-注意力机制优化" class="headerlink" title="⚡ 内容 6: 注意力机制优化"></a>⚡ 内容 6: 注意力机制优化</h2><h3 id="6-1-注意力计算复杂度分析"><a href="#6-1-注意力计算复杂度分析" class="headerlink" title="6.1 注意力计算复杂度分析"></a>6.1 注意力计算复杂度分析</h3><h4 id="标准注意力复杂度"><a href="#标准注意力复杂度" class="headerlink" title="标准注意力复杂度"></a>标准注意力复杂度</h4><p><strong>训练时（并行计算）：</strong></p>
<ul>
<li>总算术操作：$O(bnd^2)$</li>
<li>总内存访问：$O(bnd + bhn^2 + d^2)$  </li>
<li>算术强度：$O\left(\frac{1}{k} + \frac{1}{(bn)^{-1}}\right)$</li>
</ul>
<p><strong>推理时（序列生成）：</strong></p>
<ul>
<li>总算术操作：$O(bnd^2)$</li>
<li>总内存访问：$O(bn^2d + nd^2)$</li>
<li>算术强度：$O\left(\frac{n}{d} + \frac{1}{b^{-1}}\right)$</li>
</ul>
<p>其中 $b$ = batch size，$n$ = 序列长度，$d$ = 模型维度，$h$ = 注意力头数</p>
<h3 id="6-2-GQA-MQA技术"><a href="#6-2-GQA-MQA技术" class="headerlink" title="6.2 GQA/MQA技术"></a>6.2 GQA/MQA技术</h3><h4 id="MQA（多查询注意力）"><a href="#MQA（多查询注意力）" class="headerlink" title="MQA（多查询注意力）"></a>MQA（多查询注意力）</h4><p><strong>核心思想：</strong> 多个查询共享单一的键值维度</p>
<p><strong>优化后的内存访问：</strong></p>
<script type="math/tex; mode=display">O(bnd + bn^2k + nd^2)</script><p><strong>算术强度：</strong></p>
<script type="math/tex; mode=display">O\left(\frac{1}{d} + \frac{n}{dh} + \frac{1}{b^{-1}}\right)</script><p>其中 $k$ 是共享的键值维度数</p>
<h4 id="GQA（分组查询注意力）"><a href="#GQA（分组查询注意力）" class="headerlink" title="GQA（分组查询注意力）"></a>GQA（分组查询注意力）</h4><p><strong>设计：</strong> 将查询头分组，每组共享键值</p>
<p><strong>分组策略：</strong></p>
<ul>
<li>查询头数：$H_q$</li>
<li>键值头数：$H_{kv}$</li>
<li>分组大小：$G = \frac{H<em>q}{H</em>{kv}}$</li>
</ul>
<p><strong>性能对比：</strong></p>
<ul>
<li>MQA：小幅性能下降，显著内存节省</li>
<li>GQA：性能损失更小，内存节省适中</li>
</ul>
<h3 id="6-3-稀疏注意力模式"><a href="#6-3-稀疏注意力模式" class="headerlink" title="6.3 稀疏注意力模式"></a>6.3 稀疏注意力模式</h3><h4 id="滑动窗口注意力"><a href="#滑动窗口注意力" class="headerlink" title="滑动窗口注意力"></a>滑动窗口注意力</h4><p><strong>窗口大小：</strong> $W$</p>
<p><strong>注意力矩阵：</strong></p>
<script type="math/tex; mode=display">A_{ij} = \begin{cases}
\text{Attention}(q_i, k_j) & \text{if } |i-j| \leq W \\
0 & \text{otherwise}
\end{cases}</script><p><strong>复杂度：</strong> $O(nWd)$ 而非 $O(n^2d)$</p>
<h4 id="混合注意力策略"><a href="#混合注意力策略" class="headerlink" title="混合注意力策略"></a>混合注意力策略</h4><p><strong>Cohere Command A策略：</strong></p>
<ul>
<li>每4层使用一次全注意力</li>
<li>其他层使用滑动窗口注意力</li>
</ul>
<p><strong>数学表示：</strong></p>
<script type="math/tex; mode=display">\text{Layer}_i = \begin{cases}
\text{FullAttention} & \text{if } i \bmod 4 = 0 \\
\text{SlidingWindowAttention} & \text{otherwise}
\end{cases}</script><hr>
<h2 id="📋-总结与要点"><a href="#📋-总结与要点" class="headerlink" title="📋 总结与要点"></a>📋 总结与要点</h2><h3 id="🎯-架构设计的共识"><a href="#🎯-架构设计的共识" class="headerlink" title="🎯 架构设计的共识"></a>🎯 架构设计的共识</h3><ol>
<li><strong>Pre-norm几乎是标准选择</strong>，带来更好的训练稳定性</li>
<li><strong>RMSNorm在计算效率上优于LayerNorm</strong></li>
<li><strong>GLU变体通常优于标准激活函数</strong></li>
<li><strong>RoPE成为位置编码的主流选择</strong></li>
</ol>
<h3 id="📏-超参数的经验法则"><a href="#📏-超参数的经验法则" class="headerlink" title="📏 超参数的经验法则"></a>📏 超参数的经验法则</h3><ol>
<li><strong>4倍前馈比例</strong>（GLU为8/3倍）广泛适用</li>
<li><strong>头维度设计遵循简单的乘积关系</strong></li>
<li><strong>深宽比在100-200范围内表现良好</strong></li>
<li><strong>现代模型趋向减少dropout使用</strong></li>
</ol>
<h3 id="🚀-实际部署考虑"><a href="#🚀-实际部署考虑" class="headerlink" title="🚀 实际部署考虑"></a>🚀 实际部署考虑</h3><ol>
<li><strong>推理效率驱动GQA/MQA的采用</strong></li>
<li><strong>稳定性技巧对大规模训练至关重要</strong></li>
<li><strong>架构选择需要在性能和效率间平衡</strong></li>
</ol>
<h3 id="💡-学习启示"><a href="#💡-学习启示" class="headerlink" title="💡 学习启示"></a>💡 学习启示</h3><p>通过分析主流模型的实际配置，我们发现虽然存在多种架构变体，但成功的大模型在关键设计选择上表现出显著的一致性。这些经验为新模型的设计提供了宝贵的指导原则。</p>
<hr>
<h2 id="📚-参考资料"><a href="#📚-参考资料" class="headerlink" title="📚 参考资料"></a>📚 参考资料</h2><ol>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=ptFiH_bHnJw">CS336 Lecture 3 Video</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/stanford-cs336/spring2025-lectures/blob/e9cb2488fdb53ea37f0e38924ec3a1701925cef3/nonexecutable/2025%20Lecture%203%20-%20architecture.pdf">Course Materials</a></li>
<li>Su, J., et al. (2021). RoFormer: Enhanced Transformer with Rotary Position Embedding</li>
<li>Shazeer, N. (2020). GLU Variants Improve Transformer</li>
<li>Zhang, B., &amp; Sennrich, R. (2019). Root Mean Square Layer Normalization</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/05/CS336-Spring2025-Lec3/" data-id="cmcqh1lo600005alh3mvjcop1" data-title="CS336 Lecture 3:LM架构与训练 - 学习笔记" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/07/05/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-07-05T06:41:06.301Z" itemprop="datePublished">2025-07-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/07/05/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/07/05/hello-world/" data-id="cmcq77uwj0000delh13ik0wps" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/07/">July 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/07/05/CS336-Spring2025-Lec2/">CS336 Language Modeling :Tensors, Operations &amp; Training</a>
          </li>
        
          <li>
            <a href="/2025/07/05/CS336-Spring2025-Lec3/">CS336 Lecture 3:LM架构与训练 - 学习笔记</a>
          </li>
        
          <li>
            <a href="/2025/07/05/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>